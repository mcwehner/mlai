{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Information from Text\n",
    "\n",
    "*Chapter 07, NLTK: https://www.nltk.org/book/ch07.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Data\n",
    "\n",
    "#### Installing MegaM\n",
    "\n",
    "In addition to the downloaded data, [MegaM](http://legacydirs.umiacs.umd.edu/~hal/megam/index.html) is required for some classifier-based chunking.\n",
    "\n",
    "1. Download the source from http://legacydirs.umiacs.umd.edu/~hal/megam/index.html.\n",
    "2. Make the following changes to the Makefile (as needed):\n",
    "    * Update `WITHCLIBS` to point to your local caml lib dir. Invoking `ocamlc -where` may help.\n",
    "    * Change `WITHSTR` to use `-lcamlstr` instead of `lstr`.\n",
    "3. Build the optimized binary by invoking `make opt` (or `make` for the slow version).\n",
    "4. Do one of:\n",
    "    * Ensure that the location to the `megam.opt` binary is on your path.\n",
    "    * Set the environment variable `MEGAM` to the location of `megam.opt`.\n",
    "\n",
    "#### Downloading NLTK Data\n",
    "\n",
    "Use the NLTK downloader to fetch any necessary datasets and corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mcwehner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mcwehner/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package brown to /Users/mcwehner/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/mcwehner/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_sentence_segmentation(document):\n",
    "    return nltk.sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_tokenization(sentences):\n",
    "    return [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_pos_tagging(sentences):\n",
    "    return [nltk.pos_tag(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = ie_sentence_segmentation(document)\n",
    "    sentences = ie_tokenization(sentences)\n",
    "    sentences = ie_pos_tagging(sentences)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP'),\n",
      " ('has', 'VBZ'),\n",
      " ('an', 'DT'),\n",
      " ('ex', 'NN'),\n",
      " ('who', 'WP'),\n",
      " ('robbed', 'VBD'),\n",
      " ('a', 'DT'),\n",
      " ('bank', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for sentence in ie_preprocess('He has an ex who robbed a bank'):\n",
    "    pprint(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "### Noun Phrase Chunking\n",
    "\n",
    "`NP: {<DT>?<JJ>*<NN>}`: an NP chunk should be formed whenever the chunker finds an optional determiner `DT` followed by any number of adjectives `JJ` and then a noun `NN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(grammar, documents):\n",
    "    for document in documents:\n",
    "        print(document, '\\n')\n",
    "\n",
    "        for sentence in ie_preprocess(document):\n",
    "            chunk_parser = nltk.RegexpParser(grammar)\n",
    "            result       = chunk_parser.parse(sentence)\n",
    "\n",
    "            print(textwrap.indent(str(result), '\\t'), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little yellow dog barked at the cat \n",
      "\n",
      "\t(S\n",
      "\t  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "\t  barked/VBD\n",
      "\t  at/IN\n",
      "\t  (NP the/DT cat/NN)) \n",
      "\n",
      "another sharp dive \n",
      "\n",
      "\t(S (NP another/DT sharp/JJ dive/NN)) \n",
      "\n",
      "trade figures \n",
      "\n",
      "\t(S (NP trade/NN figures/NNS)) \n",
      "\n",
      "any new policy measures \n",
      "\n",
      "\t(S (NP any/DT new/JJ policy/NN measures/NNS)) \n",
      "\n",
      "earlier stages \n",
      "\n",
      "\t(S (NP earlier/RBR stages/NNS)) \n",
      "\n",
      "Panamanian dictator Manuel Noriega \n",
      "\n",
      "\t(S (NP Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP)) \n",
      "\n",
      "his Mansion House speech \n",
      "\n",
      "\t(S (NP his/PRP$ Mansion/NNP House/NNP speech/NN)) \n",
      "\n",
      "the price cutting \n",
      "\n",
      "\t(S (NP the/DT price/NN cutting/NN)) \n",
      "\n",
      "3% to 4% \n",
      "\n",
      "\t(S (NP 3/CD %/NN) to/TO (NP 4/CD %/NN)) \n",
      "\n",
      "more than 10% \n",
      "\n",
      "\t(S more/JJR than/IN (NP 10/CD %/NN)) \n",
      "\n",
      "the fastest developing trends \n",
      "\n",
      "\t(S (NP the/DT fastest/JJS developing/NN trends/NNS)) \n",
      "\n",
      "man's skill \n",
      "\n",
      "\t(S (NP man/NN) (NP 's/POS skill/NN)) \n",
      "\n",
      "the patient arrived earlier than was needed \n",
      "\n",
      "\t(S\n",
      "\t  (NP the/DT patient/NN)\n",
      "\t  arrived/VBD\n",
      "\t  earlier/JJR\n",
      "\t  than/IN\n",
      "\t  was/VBD\n",
      "\t  needed/VBN) \n",
      "\n",
      "The market for system-management software for Digital's hardware is fragmented enough that a giant such as Computer Associates should do well there. \n",
      "\n",
      "\t(S\n",
      "\t  (NP The/DT market/NN)\n",
      "\t  for/IN\n",
      "\t  (NP system-management/JJ software/NN)\n",
      "\t  for/IN\n",
      "\t  (NP Digital/NNP)\n",
      "\t  (NP 's/POS hardware/NN)\n",
      "\t  is/VBZ\n",
      "\t  fragmented/VBN\n",
      "\t  enough/RB\n",
      "\t  that/IN\n",
      "\t  a/DT\n",
      "\t  giant/JJ\n",
      "\t  such/JJ\n",
      "\t  as/IN\n",
      "\t  (NP Computer/NNP Associates/NNPS)\n",
      "\t  should/MD\n",
      "\t  do/VB\n",
      "\t  well/RB\n",
      "\t  there/RB\n",
      "\t  ./.) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = r'''\n",
    "    NP: {<DT|PRP\\$>?<JJ.*|RBR|POS>*<CD|NN.*>+}\n",
    "'''\n",
    "\n",
    "chunk(grammar, [\n",
    "    'the little yellow dog barked at the cat',\n",
    "    'another sharp dive',\n",
    "    'trade figures',\n",
    "    'any new policy measures',\n",
    "    'earlier stages',\n",
    "    'Panamanian dictator Manuel Noriega',\n",
    "    'his Mansion House speech',\n",
    "    'the price cutting',\n",
    "    '3% to 4%',\n",
    "    'more than 10%',\n",
    "    'the fastest developing trends',\n",
    "    \"man's skill\",\n",
    "    \n",
    "    'the patient arrived earlier than was needed',\n",
    "    \n",
    "    \"The market for system-management software for Digital's hardware is fragmented enough that a giant such as Computer Associates should do well there.\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapunzel let down her long golden hair \n",
      "\n",
      "\t(S\n",
      "\t  (NP Rapunzel/NNP)\n",
      "\t  let/VBD\n",
      "\t  down/RP\n",
      "\t  (NP her/PRP$ long/JJ golden/JJ hair/NN)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = r'''\n",
    "    NP: {<DT|PRP\\$>?<JJ.*>*<NN>} # determiner/possessive, adjectives, and noun\n",
    "        {<NNP>+}                 # sequences of proper nouns\n",
    "'''\n",
    "\n",
    "chunk(grammar, [\n",
    "    'Rapunzel let down her long golden hair',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Text Corpora\n",
    "\n",
    "#### `find_chunks(<grammar>, corpus=nltk.corpus.brown, limit=5)`\n",
    "\n",
    "```python\n",
    ">>> find_chunks('CHUNK: {<V.*> <TO> <V.*>}')\n",
    "```\n",
    "\n",
    "```\n",
    "(CHUNK combined/VBN to/TO achieve/VB)\n",
    "(CHUNK continue/VB to/TO place/VB)\n",
    "...\n",
    "(CHUNK wanted/VBD to/TO wait/VB)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunks(grammar, corpus=nltk.corpus.brown, limit=5):\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    \n",
    "    for sent in corpus.tagged_sents():\n",
    "        tree = cp.parse(sent)\n",
    "        \n",
    "        for subtree in tree.subtrees():\n",
    "            if 'CHUNK' == subtree.label():\n",
    "                print(subtree)\n",
    "                \n",
    "                if limit is not None:\n",
    "                    limit -= 1\n",
    "                    if limit <= 0: return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK combined/VBN to/TO achieve/VB)\n",
      "(CHUNK continue/VB to/TO place/VB)\n",
      "(CHUNK serve/VB to/TO protect/VB)\n",
      "(CHUNK wanted/VBD to/TO wait/VB)\n",
      "(CHUNK allowed/VBN to/TO place/VB)\n"
     ]
    }
   ],
   "source": [
    "find_chunks('CHUNK: {<V.*> <TO> <V.*>}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CHUNK Court/NN-TL Judge/NN-TL Durwood/NP Pye/NP)\n",
      "(CHUNK Mayor-nominate/NN-TL Ivan/NP Allen/NP Jr./NP)\n",
      "(CHUNK Georgia's/NP$ automobile/NN title/NN law/NN)\n",
      "(CHUNK State/NN-TL Welfare/NN-TL Department's/NN$-TL handling/NN)\n",
      "(CHUNK Fulton/NP-TL Tax/NN-TL Commissioner's/NN$-TL Office/NN-TL)\n"
     ]
    }
   ],
   "source": [
    "find_chunks('CHUNK: {<N(?!IL).*>{4,}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the little yellow dog barked at the cat \n",
      "\n",
      "\t(S\n",
      "\t  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "\t  barked/VBD\n",
      "\t  at/IN\n",
      "\t  (NP the/DT cat/NN)) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "grammar = r'''\n",
    "    NP: {<.*>+}     # chunk everything\n",
    "        }<VBD|IN>+{ # chink sequences of VBD and IN\n",
    "'''\n",
    "\n",
    "chunk(grammar, [\n",
    "    'the little yellow dog barked at the cat',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing and Evaluating Chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents  = nltk.corpus.conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = nltk.corpus.conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "\n",
    "def evaluate_chunker(cp):\n",
    "    print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Positive IOB tag accuracy indicates that more than a third of the words are tagged with `O`, i.e. not in an NP chunk. No chunks are found however, and precision, recall, and f-measure are therefore zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  43.4%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n"
     ]
    }
   ],
   "source": [
    "evaluate_chunker(\n",
    "    nltk.RegexpParser(''),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  87.7%%\n",
      "    Precision:     70.6%%\n",
      "    Recall:        67.8%%\n",
      "    F-Measure:     69.2%%\n"
     ]
    }
   ],
   "source": [
    "evaluate_chunker(\n",
    "    nltk.RegexpParser(r'NP: {<[CDJNP].*>+}'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram and Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents, tagger):\n",
    "        train_data  = [[(t,c) for _,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
    "        self.tagger = tagger(train_data)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags            = [pos for (_, pos) in sentence]\n",
    "        iob_tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags           = [chunktag for (_, chunktag) in iob_tagged_pos_tags]\n",
    "        conlltags           = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        \n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "evaluate_chunker(\n",
    "    TaggedChunker(train_sents, nltk.UnigramTagger),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  93.3%%\n",
      "    Precision:     82.3%%\n",
      "    Recall:        86.8%%\n",
      "    F-Measure:     84.5%%\n"
     ]
    }
   ],
   "source": [
    "evaluate_chunker(\n",
    "    TaggedChunker(train_sents, nltk.BigramTagger),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sentences):\n",
    "        train_set = []\n",
    "        \n",
    "        for tagged_sentence in train_sentences:\n",
    "            history           = []\n",
    "            untagged_sentence = nltk.tag.untag(tagged_sentence)\n",
    "            \n",
    "            for i, (_, tag) in enumerate(tagged_sentence):\n",
    "                featureset = npchunk_features(untagged_sentence, i, history)\n",
    "                \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "\n",
    "        self.classifier = nltk.MaxentClassifier.train(train_set, algorithm='megam', trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        \n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag        = self.classifier.classify(featureset)\n",
    "            \n",
    "            history.append(tag)\n",
    "        \n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunker\n",
    "\n",
    "During training, `ConsecutiveNPChunker` maps the chunk trees in the training corpus into tag sequences; in the `parse()` method, it converts the tag sequence provided by the tagger back into a chunk tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sentences):\n",
    "        tagged_sentences = [\n",
    "            [((w,t),c) for (w,t,c) in nltk.chunk.tree2conlltags(sentence)]\n",
    "            for sentence in train_sentences\n",
    "        ]\n",
    "        \n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sentences)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        tagged_sentences = self.tagger.tag(sentence)\n",
    "        conlltags        = [(w,t,c) for ((w,t),c) in tagged_sentences]\n",
    "        \n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    return { 'pos': pos }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  92.9%%\n",
      "    Precision:     79.9%%\n",
      "    Recall:        86.7%%\n",
      "    F-Measure:     83.2%%\n"
     ]
    }
   ],
   "source": [
    "evaluate_chunker(\n",
    "    ConsecutiveNPChunker(train_sents),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
