{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Information from Text\n",
    "\n",
    "*Chapter 07, NLTK: https://www.nltk.org/book/ch07.html*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Data\n",
    "\n",
    "#### Installing MegaM\n",
    "\n",
    "In addition to the downloaded data, [MegaM](http://legacydirs.umiacs.umd.edu/~hal/megam/index.html) is required for some classifier-based chunking.\n",
    "\n",
    "1. Download the source from http://legacydirs.umiacs.umd.edu/~hal/megam/index.html.\n",
    "2. Make the following changes to the Makefile (as needed):\n",
    "    * Update `WITHCLIBS` to point to your local caml lib dir. Invoking `ocamlc -where` may help.\n",
    "    * Change `WITHSTR` to use `-lcamlstr` instead of `lstr`.\n",
    "3. Build the optimized binary by invoking `make opt` (or `make` for the slow version).\n",
    "4. Do one of:\n",
    "    * Ensure that the location to the `megam.opt` binary is on your path.\n",
    "    * Set the environment variable `MEGAM` to the location of `megam.opt`.\n",
    "\n",
    "#### Downloading NLTK Data\n",
    "\n",
    "Use the NLTK downloader to fetch any necessary datasets and corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('conll2000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_sentence_segmentation(document):\n",
    "    return nltk.sent_tokenize(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_tokenization(sentences):\n",
    "    return [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_pos_tagging(sentences):\n",
    "    return [nltk.pos_tag(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = ie_sentence_segmentation(document)\n",
    "    sentences = ie_tokenization(sentences)\n",
    "    sentences = ie_pos_tagging(sentences)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "### Noun Phrase Chunking\n",
    "\n",
    "`NP: {<DT>?<JJ>*<NN>}`: an NP chunk should be formed whenever the chunker finds an optional determiner `DT` followed by any number of adjectives `JJ` and then a noun `NN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(grammar, documents):\n",
    "    for document in documents:\n",
    "        print(document, '\\n')\n",
    "\n",
    "        for sentence in ie_preprocess(document):\n",
    "            chunk_parser = nltk.RegexpParser(grammar)\n",
    "            result       = chunk_parser.parse(sentence)\n",
    "\n",
    "            print(textwrap.indent(str(result), '\\t'), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "    NP: {<DT|PRP\\$>?<JJ.*|RBR|POS>*<CD|NN.*>+}\n",
    "'''\n",
    "\n",
    "chunk(grammar, [\n",
    "    'the little yellow dog barked at the cat',\n",
    "    'another sharp dive',\n",
    "    'trade figures',\n",
    "    'any new policy measures',\n",
    "    'earlier stages',\n",
    "    'Panamanian dictator Manuel Noriega',\n",
    "    'his Mansion House speech',\n",
    "    'the price cutting',\n",
    "    '3% to 4%',\n",
    "    'more than 10%',\n",
    "    'the fastest developing trends',\n",
    "    \"man's skill\",\n",
    "    \n",
    "    'the patient arrived earlier than was needed',\n",
    "    \n",
    "    \"The market for system-management software for Digital's hardware is fragmented enough that a giant such as Computer Associates should do well there.\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "    NP: {<DT|PRP\\$>?<JJ.*>*<NN>} # determiner/possessive, adjectives, and noun\n",
    "        {<NNP>+}                 # sequences of proper nouns\n",
    "'''\n",
    "\n",
    "chunk(grammar, [\n",
    "    'Rapunzel let down her long golden hair',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Text Corpora\n",
    "\n",
    "#### `find_chunks(<grammar>, corpus=nltk.corpus.brown, limit=5)`\n",
    "\n",
    "```python\n",
    ">>> find_chunks('CHUNK: {<V.*> <TO> <V.*>}')\n",
    "```\n",
    "\n",
    "```\n",
    "(CHUNK combined/VBN to/TO achieve/VB)\n",
    "(CHUNK continue/VB to/TO place/VB)\n",
    "...\n",
    "(CHUNK wanted/VBD to/TO wait/VB)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chunks(grammar, corpus=nltk.corpus.brown, limit=5):\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    \n",
    "    for sent in corpus.tagged_sents():\n",
    "        tree = cp.parse(sent)\n",
    "        \n",
    "        for subtree in tree.subtrees():\n",
    "            if 'CHUNK' == subtree.label():\n",
    "                print(subtree)\n",
    "                \n",
    "                if limit is not None:\n",
    "                    limit -= 1\n",
    "                    if limit <= 0: return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_chunks('CHUNK: {<V.*> <TO> <V.*>}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_chunks('CHUNK: {<N(?!IL).*>{4,}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r'''\n",
    "    NP: {<.*>+}     # chunk everything\n",
    "        }<VBD|IN>+{ # chink sequences of VBD and IN\n",
    "'''\n",
    "\n",
    "chunk(grammar, [\n",
    "    'the little yellow dog barked at the cat',\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing and Evaluating Chunkers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents  = nltk.corpus.conll2000.chunked_sents('test.txt', chunk_types=['NP'])\n",
    "train_sents = nltk.corpus.conll2000.chunked_sents('train.txt', chunk_types=['NP'])\n",
    "\n",
    "def evaluate_chunker(cp):\n",
    "    print(cp.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Positive IOB tag accuracy indicates that more than a third of the words are tagged with `O`, i.e. not in an NP chunk. No chunks are found however, and precision, recall, and f-measure are therefore zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_chunker(\n",
    "    nltk.RegexpParser(''),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Regexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_chunker(\n",
    "    nltk.RegexpParser(r'NP: {<[CDJNP].*>+}'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram and Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggedChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents, tagger):\n",
    "        train_data  = [[(t,c) for _,t,c in nltk.chunk.tree2conlltags(sent)] for sent in train_sents]\n",
    "        self.tagger = tagger(train_data)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        pos_tags            = [pos for (_, pos) in sentence]\n",
    "        iob_tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags           = [chunktag for (_, chunktag) in iob_tagged_pos_tags]\n",
    "        conlltags           = [(word, pos, chunktag) for ((word, pos), chunktag) in zip(sentence, chunktags)]\n",
    "        \n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_chunker(\n",
    "    TaggedChunker(train_sents, nltk.UnigramTagger),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_chunker(\n",
    "    TaggedChunker(train_sents, nltk.BigramTagger),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "    def __init__(self, train_sentences):\n",
    "        train_set = []\n",
    "        \n",
    "        for tagged_sentence in train_sentences:\n",
    "            history           = []\n",
    "            untagged_sentence = nltk.tag.untag(tagged_sentence)\n",
    "            \n",
    "            for i, (_, tag) in enumerate(tagged_sentence):\n",
    "                featureset = npchunk_features(untagged_sentence, i, history)\n",
    "                \n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "\n",
    "        self.classifier = nltk.MaxentClassifier.train(train_set, algorithm='megam', trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        \n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag        = self.classifier.classify(featureset)\n",
    "            \n",
    "            history.append(tag)\n",
    "        \n",
    "        return zip(sentence, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chunker\n",
    "\n",
    "During training, `ConsecutiveNPChunker` maps the chunk trees in the training corpus into tag sequences; in the `parse()` method, it converts the tag sequence provided by the tagger back into a chunk tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sentences):\n",
    "        tagged_sentences = [\n",
    "            [((w,t),c) for (w,t,c) in nltk.chunk.tree2conlltags(sentence)]\n",
    "            for sentence in train_sentences\n",
    "        ]\n",
    "        \n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sentences)\n",
    "    \n",
    "    def parse(self, sentence):\n",
    "        tagged_sentences = self.tagger.tag(sentence)\n",
    "        conlltags        = [(w,t,c) for ((w,t),c) in tagged_sentences]\n",
    "        \n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    \n",
    "    return { 'pos': pos }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_chunker(\n",
    "    ConsecutiveNPChunker(train_sents),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
